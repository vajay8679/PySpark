{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b14ff6f-8869-4266-af0c-3ee01c008c93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a94b30b-3548-4e0f-85e9-30f4021bfc45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Persistence and Caching\n",
    "\n",
    "when to use -\n",
    "\n",
    "-when performing iterative operations on same dataset\n",
    "- when performing expensive operations like joins, groupBy, or aggregations\n",
    "- when you know data can be fit in memory, caching is ideal for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0296f392-7e34-4c27-8477-4725bd383b62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "spark = SparkSession.builder.appName(\"Caching and Persistance\").getOrCreate()\n",
    "\n",
    "empDf = spark.read.csv(\"dbfs:/FileStore/demo_folder/employee_dataset.csv\",header=True,inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbb696d7-f8d6-435c-a308-bd7b8c80334b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---+----------+-----------+------+------------+\n|EmployeeID|            Name|Age|Department|JoiningDate|Salary|        City|\n+----------+----------------+---+----------+-----------+------+------------+\n|         1|      Jon Rivera| 56|     Sales| 2024-04-29|121250|     Houston|\n|         2|  Nicole Daniels| 46|        HR| 2024-04-01|138633|    New York|\n|         3|Monique Sullivan| 32|   Finance| 2020-05-02| 83619|Philadelphia|\n|         4|    James Wright| 60| Marketing| 2023-02-21|129751|    New York|\n|         5| Nicole Williams| 25|     Sales| 2018-04-29|123193|     Chicago|\n|         6|     David Bates| 38|     Sales| 2019-03-12| 98719|      Dallas|\n|         7|   Matthew Riggs| 56| Marketing| 2019-11-06| 71156| Los Angeles|\n|         8|    Wendy Powers| 36|Operations| 2023-07-30| 73901|      Dallas|\n|         9|  Thomas Collins| 40|   Support| 2018-10-12| 30418|Philadelphia|\n|        10|     Joshua Wong| 28|        IT| 2022-05-17| 89252| San Antonio|\n+----------+----------------+---+----------+-----------+------+------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "empDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0900a85d-9af3-4518-9324-b3b97ece5ab9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transformed_df = empDf.filter(empDf.Salary > 90000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9cbdd6b-f5cc-4983-869c-12fb7b5e692f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: DataFrame[EmployeeID: int, Name: string, Age: int, Department: string, JoiningDate: date, Salary: int, City: string]"
     ]
    }
   ],
   "source": [
    "transformed_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbf3d597-3e2e-4cc5-b417-46642ca9f1cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count after cache: 4998626\n"
     ]
    }
   ],
   "source": [
    "print(\"Count after cache:\",transformed_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72898a36-b64e-4848-812f-02dc04ab98f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---+----------+-----------+------+------------+\n|EmployeeID|                Name|Age|Department|JoiningDate|Salary|        City|\n+----------+--------------------+---+----------+-----------+------+------------+\n|         1|          Jon Rivera| 56|     Sales| 2024-04-29|121250|     Houston|\n|         2|      Nicole Daniels| 46|        HR| 2024-04-01|138633|    New York|\n|         4|        James Wright| 60| Marketing| 2023-02-21|129751|    New York|\n|         5|     Nicole Williams| 25|     Sales| 2018-04-29|123193|     Chicago|\n|         6|         David Bates| 38|     Sales| 2019-03-12| 98719|      Dallas|\n|        11|      Dakota Shields| 28|        HR| 2018-02-24|114528|      Dallas|\n|        12|       Angelica Wade| 41|        HR| 2015-06-29|130914|     Phoenix|\n|        13|      Allison Miller| 53| Marketing| 2019-09-19|108950|      Dallas|\n|        14|        Ryan Morales| 57| Marketing| 2021-05-13|130007|      Dallas|\n|        15|    Elizabeth Lawson| 41|Operations| 2022-11-12| 94927| Los Angeles|\n|        19|   Gilbert Fernandez| 41|   Finance| 2015-02-02|121576|    New York|\n|        23|      Cindy Delacruz| 19|Operations| 2017-02-21|137989|     Phoenix|\n|        27|       Michael Henry| 39|        HR| 2015-01-07|102593|     Phoenix|\n|        28|    Melissa Marshall| 61|     Sales| 2022-02-28|130505|Philadelphia|\n|        30|    Natalie Alvarado| 44|        IT| 2018-06-05|147167| Los Angeles|\n|        35|     David Jefferson| 64|   Finance| 2016-11-29|144664|     Houston|\n|        37|    Ethan Williamson| 20|        HR| 2018-06-11|110640|     Phoenix|\n|        38|    Katherine Archer| 54|   Support| 2015-02-27|125972|     Phoenix|\n|        39|       Melissa Davis| 24|        HR| 2015-09-02| 97643|    New York|\n|        41|Christopher Cruz DDS| 26| Marketing| 2016-11-22|144849|      Dallas|\n+----------+--------------------+---+----------+-----------+------+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3068590-1971-4d2f-99de-2a8cb5eb60e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = transformed_df.groupBy(\"Department\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7706c926-7284-487c-9958-2d07b1036b08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n|Department| count|\n+----------+------+\n|     Sales|714347|\n|        HR|715662|\n|   Finance|713291|\n| Marketing|713757|\n|        IT|714311|\n|   Support|713916|\n|Operations|713342|\n+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9520aef9-f1f6-492c-9ace-00edb1afbae9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: DataFrame[EmployeeID: int, Name: string, Age: int, Department: string, JoiningDate: date, Salary: int, City: string]"
     ]
    }
   ],
   "source": [
    "transformed_df.unpersist() #Release the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2215858-62ad-44c0-b079-7caf01171565",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop() #stop the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f9fcc84-133c-4a9b-9a4c-366645748afa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#persist()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "spark = SparkSession.builder.appName(\"Persistance example\").getOrCreate()\n",
    "\n",
    "data = [(\"James\",35),(\"Aman\",30),(\"Michel\",29),(\"Sarah\",25)]\n",
    "columns = [\"Name\",\"Age\"]\n",
    "\n",
    "df = spark.createDataFrame(data,columns)\n",
    "\n",
    "transformed_df = df.filter(df.Age > 28)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a1d8d38-15df-4540-b72e-750340b24f85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: DataFrame[Name: string, Age: bigint]"
     ]
    }
   ],
   "source": [
    "transformed_df.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d91c00b-8e66-4e51-8815-6ac558376499",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count after persist: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Count after persist:\",transformed_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ded87894-317c-42a5-ab0b-f08be38ae939",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n|  Name|Age|\n+------+---+\n| James| 35|\n|  Aman| 30|\n|Michel| 29|\n+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58eea4e6-28aa-4761-a8de-e66231b1187f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "group_df = transformed_df.groupBy(\"Age\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45ba15fb-90c8-4d70-9dda-5ec895f28fcc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n|Age|count|\n+---+-----+\n| 35|    1|\n| 30|    1|\n| 29|    1|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "group_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c761bba3-c4f3-4cfe-8a74-edbd2965f3b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: DataFrame[Name: string, Age: bigint]"
     ]
    }
   ],
   "source": [
    "transformed_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e161f1d1-1db0-4a71-868e-b85889c29288",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d63b1da2-f552-4c81-9807-204d96425c7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Broadcast \n",
    "\n",
    "how it works\n",
    "\n",
    "- Instend of sending both datasets to all nodes for shuffling and merging, spark sends the smaller dataset(broadcasted) to each executor\n",
    "-This reduces netwrok traffic because only the smaller dataset is distributed and the large dataset remain in inplace, avoding a costly shuffle of the large data  \n",
    "\n",
    "When to use Broadcast joins:\n",
    "-when one dataset is much smaller than the other\n",
    "-when the smaller dataset can fit into the memory executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaa57393-f1fa-499e-bb21-0675092396f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Broadcast Join Example\").getOrCreate()\n",
    "df_large = spark.createDataFrame([(1,'2024-01-01',100),\n",
    "                                  (2,'2024-01-02',150),\n",
    "                                  (3,'2024-01-03',200)],[\"product_id\",\"transaction_date\",\"amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "864b2b4b-2bdf-430b-a5ce-683ca4047a49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_small = spark.createDataFrame([(1,\"Product A\",\"Category 1\"),\n",
    "                                  (2,\"Product B\",\"Category 2\"),\n",
    "                                  (3,\"Product C\",\"Category 3\")],[\"product_id\",\"product_name\",\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fe6fcf4-25ad-495b-af07-c4a78dd332d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",\"20MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1716c5cb-f667-4ecd-bd47-6f47f3e03caf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_joined = df_large.join(broadcast(df_small),on='product_id',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43603106-20f5-488c-91fc-724faa336282",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+------+------------+----------+\n|product_id|transaction_date|amount|product_name|  category|\n+----------+----------------+------+------------+----------+\n|         1|      2024-01-01|   100|   Product A|Category 1|\n|         2|      2024-01-02|   150|   Product B|Category 2|\n|         3|      2024-01-03|   200|   Product C|Category 3|\n+----------+----------------+------+------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8a6c6dc-51e0-4910-a7b9-d945f68ebbf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Data skew - salting\n",
    "\n",
    "How to Detect Data Skew:\n",
    "Skewed Partitions: Some partitions may contain significantly more data than others.\n",
    "Stragglers: Certain tasks take much longer to complete compared to others.\n",
    "Skew in Keys: When joining or aggregating on a key, a few keys may have a disproportionately large number of records.\n",
    "Strategies to Avoid Skew in Spark Jobs\n",
    "\n",
    "-Salting the Keys\n",
    "One common approach to avoid skew in joins or aggregations is salting. When one key has a large number of records, you artificially split 1f9c2_salt that key into multiple sub-keys, which helps distribute the load evenly across partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a64a25de-d731-47fd-9ad1-5d68ad31a95c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F,SparkSession\n",
    "import random\n",
    "spark = SparkSession.builder.appName(\"Salting Example\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39866460-5c95-4a20-aaab-79c3d84bfc2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_large = spark.createDataFrame([(1,'2024-01-01',100),\n",
    "                                  (2,'2024-01-02',150),\n",
    "                                  (3,'2024-01-03',200)],[\"product_id\",\"transaction_date\",\"amount\"])\n",
    "\n",
    "df_small = spark.createDataFrame([(1,\"Product A\",\"Category 1\"),\n",
    "                                  (2,\"Product B\",\"Category 2\"),\n",
    "                                  (3,\"Product C\",\"Category 3\")],[\"product_id\",\"product_name\",\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84f424ab-6439-44ce-8243-f3917824d4ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add salt to large dataframe (df_arge) by creating a random number as salt\n",
    "df_large = df_large.withColumn(\"salt\",F.lit((random.randint(0,2))))\n",
    "\n",
    "#Add the same salt column in df_small by cross-joining with the salt values\n",
    "df_small =df_small.crossJoin(spark.createDataFrame([(0,),(1,),(2,)],[\"salt\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "330999e7-eb47-4d4d-856c-5c9b4689bc7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_joined = df_large.join(df_small,on=['product_id','salt'],how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfea835e-a18f-4998-8070-324dcf17d99b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------------+------+------------+----------+\n|product_id|salt|transaction_date|amount|product_name|  category|\n+----------+----+----------------+------+------------+----------+\n|         1|   2|      2024-01-01|   100|   Product A|Category 1|\n|         2|   2|      2024-01-02|   150|   Product B|Category 2|\n|         3|   2|      2024-01-03|   200|   Product C|Category 3|\n+----------+----+----------------+------+------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_joined.show()\n",
    "\n",
    "# Explanation:\n",
    "# We artificially introduced a salt column to distribute the product_id key more evenly.\n",
    "# The cross join with the salt column ensures that for each product_id in df_small, we have multiple salted versions to match the larger dataset.\n",
    "# This breaks up the skewed key into smaller pieces to spread the load across multiple partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c22e1f-1d3f-4a15-9594-fb541b4987b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3953fa2d-a009-4958-8f04-09f6b4a0d905",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark5.5",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
